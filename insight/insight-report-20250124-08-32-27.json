{"amount_correct": 6, "percentage_score": 30, "report_time": "2025-01-24 13:32:26", "checks": [{"description": "Ensure that the README.md file exists inside of the root of the GitHub repository", "check": "ConfirmFileExists", "status": true, "path": "../README.md"}, {"description": "Delete the phrase 'Add Your Name Here' and add your own name as an Honor Code pledge in README.md", "check": "MatchFileFragment", "options": {"fragment": "Add Your Name Here", "count": 0, "exact": true}, "status": true, "path": "../README.md"}, {"description": "Complete all TODOs, remove the TODO markers, and rewrite comments for README.md", "check": "MatchFileFragment", "options": {"fragment": "TODO", "count": 0, "exact": true}, "status": false, "path": "../README.md", "diagnostic": "Found 5 fragment(s) in the README.md or the output while expecting exactly 0"}, {"description": "Retype the every word in the Honor Code pledge in README.md", "check": "MatchFileFragment", "options": {"fragment": "I adhered to the Allegheny College Honor Code while completing this algorithm engineering skill check.", "count": 3, "exact": true}, "status": false, "path": "../README.md", "diagnostic": "Found 2 fragment(s) in the README.md or the output while expecting exactly 3"}, {"description": "Indicate that you have completed all of the tasks in the README.md", "check": "MatchFileFragment", "options": {"fragment": "- [X]", "count": 10, "exact": true}, "status": false, "path": "../README.md", "diagnostic": "Found 0 fragment(s) in the README.md or the output while expecting exactly 10"}, {"description": "Ensure that question_one.py file exists in the questions/ directory", "check": "ConfirmFileExists", "status": true, "path": "questions/question_one.py"}, {"description": "Complete all TODOs, remove the TODO markers, and rewrite comments for question_one.py", "check": "MatchFileFragment", "options": {"fragment": "TODO", "count": 0, "exact": true}, "status": false, "path": "questions/question_one.py", "diagnostic": "Found 9 fragment(s) in the question_one.py or the output while expecting exactly 0"}, {"description": "Create a sufficient number of docstring (i.e., multiple-line) comments in question_one.py", "check": "CountMultipleLineComments", "options": {"language": "Python", "count": 5, "exact": false}, "status": false, "path": "questions/question_one.py", "diagnostic": "Found 3 comment(s) in the question_one.py or the output"}, {"description": "Create a sufficient number of single-line comments in question_one.py", "check": "CountSingleLineComments", "options": {"language": "Python", "count": 20, "exact": false}, "status": true, "path": "questions/question_one.py"}, {"description": "Ensure that test_question_one.py file exists in the tests/ directory", "check": "ConfirmFileExists", "status": true, "path": "tests/test_question_one.py"}, {"description": "Run checks for Question 1 Part (a) with 'execexam' command and confirm correct exit code", "command": "poetry run execexam . tests/ --mark \"question_one_part_a\" --no-fancy --report status --report trace --report failure --report setup --report code", "status": false, "diagnostic": "Parameter Information\n     \n     - project: .\n     - tests: tests\n     - tldr: None\n     - report: [<ReportType.exitcode: 'status'>, <ReportType.testtrace: 'trace'>, <ReportType.testfailures: 'failure'>, \n     <ReportType.setup: 'setup'>, <ReportType.testcodes: 'code'>]\n     - mark: question_one_part_a\n     - maxfail: 10\n     - advice_method: AdviceMethod.api_key\n     - advice_model: None\n     - advice_server: None\n     - debug: False\n     - fancy: False\n     - syntax_theme: Theme.ansi_dark\n     - return_code: 0\n     - litellm_thread: <Thread(Thread-1 (load_litellm), initial)>\n     - display_report_type: ReportType.testadvice\n     - json_report_plugin: <pytest_jsonreport.plugin.JSONReport object at 0x7f333029ed50>\n     \n     Test Trace\n     \n     FAILED tests/test_question_one.py::test_detect_duplicates - assert False\n     \n     test_question_one.py::test_detect_duplicates\n       - Status: Failed\n         Line: 19\n         Exact: False ...\n         Message: AssertionError\n     \n     Test Failure(s)\n     \n       Name: tests/test_question_one.py::test_detect_duplicates\n       Path: <...>/algorithm-skill-check-one-starter/exam/tests/test_question_one.py\n       Line number: 19\n       Message: assert False\n      +  where False = detect_duplicates([1, 2, 3, 4, 1])\n     \n     Failing Test\n     \n     # File: tests/test_question_one.py Line: 16\n     @pytest.mark.question_one_part_a\n     def test_detect_duplicates():\n         \"\"\"Confirm correctness of question part.\"\"\"\n         assert detect_duplicates([1, 2, 3, 4, 1])\n         assert not detect_duplicates([1, 2, 3, 4, 5])\n         assert detect_duplicates([1, 1, 1, 1, 1])\n         assert not detect_duplicates([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n         assert not detect_duplicates([])\n         assert not detect_duplicates([1])\n         assert detect_duplicates([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1])\n         assert not detect_duplicates([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11])\n     \n     Overall Status\n     \n     \u2718 One or more checks failed."}, {"description": "Run checks for Question 1 Part (b) with 'execexam' command and confirm correct exit code", "command": "poetry run execexam . tests/ --mark \"question_one_part_b\" --no-fancy --report status --report trace --report failure --report setup --report code", "status": false, "diagnostic": "Parameter Information\n     \n     - project: .\n     - tests: tests\n     - tldr: None\n     - report: [<ReportType.exitcode: 'status'>, <ReportType.testtrace: 'trace'>, <ReportType.testfailures: 'failure'>, \n     <ReportType.setup: 'setup'>, <ReportType.testcodes: 'code'>]\n     - mark: question_one_part_b\n     - maxfail: 10\n     - advice_method: AdviceMethod.api_key\n     - advice_model: None\n     - advice_server: None\n     - debug: False\n     - fancy: False\n     - syntax_theme: Theme.ansi_dark\n     - return_code: 0\n     - litellm_thread: <Thread(Thread-1 (load_litellm), initial)>\n     - display_report_type: ReportType.testadvice\n     - json_report_plugin: <pytest_jsonreport.plugin.JSONReport object at 0x7f97f03e9a60>\n     \n     Test Trace\n     \n     FAILED tests/test_question_one.py::test_detect_duplicates_in_lists - assert [True] == [True, False]\n     \n     test_question_one.py::test_detect_duplicates_in_lists\n       - Status: Failed\n         Line: 32\n         Exact: [True] == [True, False] ...\n         Message: AssertionError\n     \n     Test Failure(s)\n     \n       Name: tests/test_question_one.py::test_detect_duplicates_in_lists\n       Path: <...>/algorithm-skill-check-one-starter/exam/tests/test_question_one.py\n       Line number: 32\n       Message: assert [True] == [True, False]\n       \n       Right contains one more item: False\n       \n       Full diff:\n         [\n             True,\n       -     False,\n         ]\n     \n     Failing Test\n     \n     # File: tests/test_question_one.py Line: 29\n     @pytest.mark.question_one_part_b\n     def test_detect_duplicates_in_lists():\n         \"\"\"Confirm correctness of question part.\"\"\"\n         assert detect_duplicates_in_lists([[1, 2, 3, 4, 1], [1, 2, 3, 4, 5]]) == [\n             True,\n             False,\n         ]\n         assert detect_duplicates_in_lists(\n             [[1, 1, 1, 1, 1], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2]]\n         ) == [True, True]\n         assert detect_duplicates_in_lists([[], [1]]) == [False, False]\n         assert detect_duplicates_in_lists(\n             [\n                 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n                 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 5, 11],\n             ]\n         ) == [False, True]\n     \n     Overall Status\n     \n     \u2718 One or more checks failed."}, {"description": "Run checks for Question 1 Part (c) with 'execexam' command and confirm correct exit code", "command": "poetry run execexam . tests/ --mark \"question_one_part_c\" --no-fancy --report status --report trace --report failure --report setup --report code", "status": false, "diagnostic": "Parameter Information\n     \n     - project: .\n     - tests: tests\n     - tldr: None\n     - report: [<ReportType.exitcode: 'status'>, <ReportType.testtrace: 'trace'>, <ReportType.testfailures: 'failure'>, \n     <ReportType.setup: 'setup'>, <ReportType.testcodes: 'code'>]\n     - mark: question_one_part_c\n     - maxfail: 10\n     - advice_method: AdviceMethod.api_key\n     - advice_model: None\n     - advice_server: None\n     - debug: False\n     - fancy: False\n     - syntax_theme: Theme.ansi_dark\n     - return_code: 0\n     - litellm_thread: <Thread(Thread-1 (load_litellm), initial)>\n     - display_report_type: ReportType.testadvice\n     - json_report_plugin: <pytest_jsonreport.plugin.JSONReport object at 0x7f6da1396c90>\n     \n     Test Trace\n     \n     FAILED tests/test_question_one.py::test_detect_duplicates_in_dict - AssertionError: assert False\n     \n     test_question_one.py::test_detect_duplicates_in_dict\n       - Status: Failed\n         Line: 51\n         Exact: False ...\n         Message: AssertionError\n     \n     Test Failure(s)\n     \n       Name: tests/test_question_one.py::test_detect_duplicates_in_dict\n       Path: <...>/algorithm-skill-check-one-starter/exam/tests/test_question_one.py\n       Line number: 51\n       Message: AssertionError: assert False\n      +  where False = detect_duplicates_in_dict({'a': '1', 'b': '2', 'c': '1'})\n     \n     Failing Test\n     \n     # File: tests/test_question_one.py Line: 48\n     @pytest.mark.question_one_part_c\n     def test_detect_duplicates_in_dict():\n         \"\"\"Confirm correctness of question part.\"\"\"\n         assert detect_duplicates_in_dict({\"a\": \"1\", \"b\": \"2\", \"c\": \"1\"})\n         assert not detect_duplicates_in_dict({\"a\": \"1\", \"b\": \"2\", \"c\": \"3\"})\n         assert detect_duplicates_in_dict({\"a\": \"1\", \"b\": \"1\", \"c\": \"1\"})\n         assert not detect_duplicates_in_dict(\n             {\"a\": \"1\", \"b\": \"2\", \"c\": \"3\", \"d\": \"4\"}\n         )\n         assert not detect_duplicates_in_dict({})\n         assert not detect_duplicates_in_dict({\"a\": \"1\"})\n     \n     Overall Status\n     \n     \u2718 One or more checks failed."}, {"description": "Run checks for Question 1 Part (d) with 'execexam' command and confirm correct exit code", "command": "poetry run execexam . tests/ --mark \"question_one_part_d\" --no-fancy --report status --report trace --report failure --report setup --report code", "status": false, "diagnostic": "Parameter Information\n     \n     - project: .\n     - tests: tests\n     - tldr: None\n     - report: [<ReportType.exitcode: 'status'>, <ReportType.testtrace: 'trace'>, <ReportType.testfailures: 'failure'>, \n     <ReportType.setup: 'setup'>, <ReportType.testcodes: 'code'>]\n     - mark: question_one_part_d\n     - maxfail: 10\n     - advice_method: AdviceMethod.api_key\n     - advice_model: None\n     - advice_server: None\n     - debug: False\n     - fancy: False\n     - syntax_theme: Theme.ansi_dark\n     - return_code: 0\n     - litellm_thread: <Thread(Thread-1 (load_litellm), initial)>\n     - display_report_type: ReportType.testadvice\n     - json_report_plugin: <pytest_jsonreport.plugin.JSONReport object at 0x7f476f8e9a60>\n     \n     Test Trace\n     \n     FAILED tests/test_question_one.py::test_detect_duplicates_below_threshold_dict - AssertionError: assert not True\n     \n     test_question_one.py::test_detect_duplicates_below_threshold_dict\n       - Status: Passed\n         Line: 64\n         Code: detect_duplicates_below_threshold_in_dict(\n             {\"a\": \"1\", \"b\": \"2\", \"c\": \"1\"}, 2\n         )\n         Exact: True ...\n       - Status: Passed\n         Line: 67\n         Code: detect_duplicates_below_threshold_in_dict(\n             {\"a\": \"1\", \"b\": \"2\", \"c\": \"1\"}, 1\n         )\n         Exact: True ...\n       - Status: Passed\n         Line: 70\n         Code: detect_duplicates_below_threshold_in_dict(\n             {\"a\": \"1\", \"b\": \"2\", \"c\": \"3\"}, 1\n         )\n         Exact: True ...\n       - Status: Passed\n         Line: 73\n         Code: detect_duplicates_below_threshold_in_dict(\n             {\"a\": \"1\", \"b\": \"2\", \"c\": \"3\"}, 0\n         )\n         Exact: True ...\n       - Status: Passed\n         Line: 76\n         Code: detect_duplicates_below_threshold_in_dict(\n             {\"a\": \"1\", \"b\": \"1\", \"c\": \"1\"}, 2\n         )\n         Exact: True ...\n       - Status: Failed\n         Line: 79\n         Exact: not True ...\n         Message: AssertionError\n     \n     Test Failure(s)\n     \n       Name: tests/test_question_one.py::test_detect_duplicates_below_threshold_dict\n       Path: <...>/algorithm-skill-check-one-starter/exam/tests/test_question_one.py\n       Line number: 79\n       Message: AssertionError: assert not True\n      +  where True = detect_duplicates_below_threshold_in_dict({'a': '1', 'b': '1', 'c': '1'}, 1)\n     \n     Failing Test\n     \n     # File: tests/test_question_one.py Line: 61\n     @pytest.mark.question_one_part_d\n     def test_detect_duplicates_below_threshold_dict():\n         \"\"\"Confirm correctness of question part.\"\"\"\n         assert detect_duplicates_below_threshold_in_dict(\n             {\"a\": \"1\", \"b\": \"2\", \"c\": \"1\"}, 2\n         )\n         assert detect_duplicates_below_threshold_in_dict(\n             {\"a\": \"1\", \"b\": \"2\", \"c\": \"1\"}, 1\n         )\n         assert detect_duplicates_below_threshold_in_dict(\n             {\"a\": \"1\", \"b\": \"2\", \"c\": \"3\"}, 1\n         )\n         assert detect_duplicates_below_threshold_in_dict(\n             {\"a\": \"1\", \"b\": \"2\", \"c\": \"3\"}, 0\n         )\n         assert detect_duplicates_below_threshold_in_dict(\n             {\"a\": \"1\", \"b\": \"1\", \"c\": \"1\"}, 2\n         )\n         assert not detect_duplicates_below_threshold_in_dict(\n             {\"a\": \"1\", \"b\": \"1\", \"c\": \"1\"}, 1\n         )\n         assert detect_duplicates_below_threshold_in_dict(\n             {\"a\": \"1\", \"b\": \"1\", \"c\": \"2\", \"d\": \"3\", \"e\": \"3\"}, 3\n         )\n         assert detect_duplicates_below_threshold_in_dict(\n             {\"a\": \"1\", \"b\": \"1\", \"c\": \"2\", \"d\": \"3\", \"e\": \"3\"}, 2\n         )\n         assert not detect_duplicates_below_threshold_in_dict(\n             {\"a\": \"1\", \"b\": \"1\", \"c\": \"2\", \"d\": \"3\", \"e\": \"3\"}, 1\n         )\n         assert detect_duplicates_below_threshold_in_dict({}, 1)\n         assert detect_duplicates_below_threshold_in_dict({}, 0)\n         assert detect_duplicates_below_threshold_in_dict({\"a\": \"1\"}, 1)\n         assert detect_duplicates_below_threshold_in_dict({\"a\": \"1\"}, 0)\n     \n     Overall Status\n     \n     \u2718 One or more checks failed."}, {"description": "Ensure that Question 1 follows industry-standard rules using the command 'ruff check'", "command": "poetry run ruff check questions/question_one.py", "status": false, "diagnostic": "questions/question_one.py:8:20: F401 [*] `typing.Any` imported but unused\n        |\n      6 | # to the industry best practices for Python source code.\n      7 |\n      8 | from typing import Any, Dict, List\n        |                    ^^^ F401\n      9 |\n     10 | # Introduction: Read This First! {{{\n        |\n        = help: Remove unused import\n     \n     questions/question_one.py:8:31: F401 [*] `typing.List` imported but unused\n        |\n      6 | # to the industry best practices for Python source code.\n      7 |\n      8 | from typing import Any, Dict, List\n        |                               ^^^^ F401\n      9 |\n     10 | # Introduction: Read This First! {{{\n        |\n        = help: Remove unused import\n     \n     Found 2 errors.\n     [*] 2 fixable with the `--fix` option."}, {"description": "Ensure that Question 1 adheres to an industry-standard format using the command 'ruff format'", "command": "poetry run ruff format questions/question_one.py --check", "status": false, "diagnostic": "Would reformat: questions/question_one.py\n     1 file would be reformatted"}, {"description": "Ensure that Question 1 has correct type annotations using the command 'mypy'", "command": "poetry run mypy questions/question_one.py", "status": true}, {"description": "Ensure that Question 1 has correct number of fully type annotated functions using the command 'symbex'", "check": "MatchCommandFragment", "options": {"command": "poetry run symbex -s --fully-typed -f questions/question_one.py --count", "fragment": 4, "count": 1, "exact": true}, "status": false, "diagnostic": "Found 0 fragment(s) in the file or the output while expecting exactly 1"}, {"description": "Ensure that Question 1 has correct number of documented functions using the command 'symbex'", "check": "MatchCommandFragment", "options": {"command": "poetry run symbex -s --documented -f questions/question_one.py --count", "fragment": 4, "count": 1, "exact": true}, "status": false, "diagnostic": "Found 0 fragment(s) in the file or the output while expecting exactly 1"}, {"description": "Ensure that Question 1 has no undocumented functions using the command 'symbex'", "check": "MatchCommandFragment", "options": {"command": "poetry run symbex -s --undocumented -f questions/question_one.py --count", "fragment": 0, "count": 1, "exact": true}, "status": false, "diagnostic": "Found 0 fragment(s) in the file or the output while expecting exactly 1"}]}